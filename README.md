---
title: Google Ads Policy RAG
emoji: ğŸ”
colorFrom: blue
colorTo: purple
sdk: streamlit
sdk_version: 1.31.0
app_file: app.py
pinned: false
---

# Google Ads Policy RAG System

A **production-grade Retrieval-Augmented Generation (RAG) system** for interpreting, enforcing, and evaluating **Google Ads policies** with citations, confidence scoring, and human-in-the-loop escalation.

---

## ğŸ¯ Project Goal

Build an AI-powered assistant that can:

* Answer questions about Google Ads policies
* Review ad text for policy compliance
* Cite the **exact policy sections** used in decisions
* Quantify confidence and **escalate ambiguous cases** for human review

This system is designed for **policy QA, compliance tooling, and trust & safety workflows**.

---

## ğŸ—ï¸ System Architecture

```
User Query / Ad Text
        â†“
Dense Embeddings (BGE-large)
        â†“
Hybrid Retrieval (FAISS + BM25)
        â†“
Cross-Encoder Reranking
        â†“
LLM Policy Decision + Confidence
        â†“
Escalation / Citation Output
```

**Design principle:** explicit, modular pipelines over black-box frameworks.

---

## ğŸ§  Design Philosophy (Why No LangChain?)

This project intentionally avoids heavy RAG frameworks (e.g., LangChain / LangGraph) in favor of **custom Python pipelines**, enabling:

* Full control over retrieval, ranking, and scoring
* Transparent evaluation with RAGAS
* Easier debugging and profiling
* Production-aligned system design

Frameworks can be layered later if needed â€” the core logic is framework-agnostic.

---

## ğŸ› ï¸ Tech Stack

* **Embeddings**: BGE-large-en-v1.5 (1024-dim)
* **Vector Store**: FAISS (cosine similarity)
* **Sparse Retrieval**: BM25
* **Reranker**: Cross-Encoder
* **LLM**: Gemini (free tier)
* **Evaluation**: RAGAS
* **Language**: Python

---

## ğŸ“ Project Structure

```
google-ads-policy-rag/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Scraped HTML pages (not in git)
â”‚   â”œâ”€â”€ processed/        # Parsed sections & chunks
â”‚   â””â”€â”€ embeddings/       # Vector embeddings (not in git)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/        # Scraping, parsing, chunking
â”‚   â”œâ”€â”€ retrieval/        # FAISS, BM25, hybrid search
â”‚   â”œâ”€â”€ generation/       # LLM prompts & decision logic
â”‚   â””â”€â”€ evaluation/       # RAGAS evaluation
â”œâ”€â”€ run_phase1_DataIngestion.py
â”œâ”€â”€ run_phase2_Embeddings_VectorStore.py
â”œâ”€â”€ run_phase3_RetrievalRanking.py
â”œâ”€â”€ run_phase4_Generation.py
â””â”€â”€ tests/
```

---

## ğŸ“Š Implementation Status

* **Phase 1**: Ingestion & Hierarchical Chunking âœ…
* **Phase 2**: Dense + Sparse Retrieval âœ…
* **Phase 3**: Reranking & LLM Decisioning âœ…
* **Phase 4**: RAGAS Evaluation & Metrics âœ…

---

## ğŸ“ˆ Final Production Metrics

### ğŸ¯ Core Performance
```
| Metric             | Value     | Grade | Notes                             |
| ------------------ | --------- | ----- | --------------------------------- |
| Decision Accuracy  | **80.0%** | A-    | 8/10 correct decisions            |
| Retrieval Recall@5 | **77.8%** | B+    | Correct policy found 7/9 times    |
| MRR                | **0.778** | B+    | Avg correct rank: 1.3             |
| Policy Match       | **66.7%** | C+    | Exact section cited               |
| Confidence Score   | **29.2%** | D     | Conservative calibration          |
| Escalation Rate    | **100%**  | â€”     | All flagged for review (expected) |
```
---

### â±ï¸ Pipeline Performance
```
| Phase                  | Time     | Throughput     | Status |
| ---------------------- | -------- | -------------- | ------ |
| Scraping (25 pages)    | 39.4s    | 0.6 pages/s    | âœ…      |
| Parsing (236 sections) | 0.6s     | 393 sections/s | âœ…      |
| Chunking (341 chunks)  | 0.2s     | 1,705 chunks/s | âœ…      |
| Embeddings (341)       | 23.5s    | 14.5 vec/s     | âœ…      |
| FAISS Index            | ~0s      | Instant        | âœ…      |
| BM25 Index             | 0.02s    | Instant        | âœ…      |
| **Total Pipeline**     | **~64s** | ~5 chunks/s    | âœ…      |
```
---

### ğŸ” Search & Decision Latency
```
| Stage           | Time | Notes              |
| --------------- | ---- | ------------------ |
| Semantic Search | ~10s | Model load + FAISS |
| BM25 Search     | <1s  | Very fast          |
| Hybrid + Rerank | ~27s | Cross-encoder load |
| LLM Decision    | ~53s | Gemini API (3 ads) |
```
---

### ğŸ’¾ Data Footprint
```
| Component    | Size    | Status     |
| ------------ | ------- | ---------- |
| Raw HTML     | ~50 MB  | Not in git |
| Chunked JSON | 356 KB  | âœ…          |
| Embeddings   | 1.33 MB | Not in git |
| FAISS Index  | 1.33 MB | Not in git |
| BM25 Index   | ~500 KB | Not in git |
```
---

## âœ… Key Achievements

* **80% decision accuracy** â€” suitable for assisted review workflows
* **Hybrid retrieval** improves recall over dense-only search
* **Sub-minute end-to-end pipeline** for policy ingestion
* **Clean chunking** (no UI junk like â€œWas this helpful?â€)
* **Zero-cost inference** using Gemini free tier

---

## âš ï¸ Known Limitations

* **Low confidence calibration** (29%) â€” overly conservative
* **Low reranker scores** on complex policy queries
* **100% escalation rate** â€” needs confidence threshold tuning
* **Gemini rate limits** on free tier

These are expected tradeoffs for a safety-first system.

---

## ğŸš€ Quick Start

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Phase 1: Ingestion
python run_phase1_DataIngestion.py

# Phase 2: Retrieval
python run_phase2_Embeddings_VectorStore.py

# Phase 3: Generation
python run_phase3_RetrievalRanking.py

# Phase 4: Evaluation
python run_phase4_Generation.py
```

---

## ğŸ§ª Evaluation

Evaluation is performed using **RAGAS**, measuring:

* Faithfulness
* Answer relevance
* Context recall
* Policy grounding

Results are stored in:

```
evaluation/evaluation_results.json
```

---

## ğŸ“Œ Future Improvements

* Confidence calibration & threshold tuning
* Query rewriting for complex ads
* Policy section-level supervision
* Streaming inference & caching
* UI for reviewer workflows
* LangChain/ LangGraph

---

## ğŸ“ License

MIT

---

## ğŸ‘¤ Author

**Varun Vaddi**
MS in Data Science, University of Houston
Focus: RAG systems, policy AI, trust & safety
